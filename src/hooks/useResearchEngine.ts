import { useState, useCallback } from 'react';
import { ResearchTask, ResearchResult, Report, useResearchStore } from '@/store/researchStore';
import { researchApi } from '@/lib/api/research';
import { toast } from '@/hooks/use-toast';

export const useResearchEngine = () => {
  const { 
    addTask, 
    updateTask, 
    addReport, 
    setIsSearching,
    setSearchQuery,
    deepVerifyMode,
    deepVerifySourceConfigs,
    setDeepVerifySources,
    updateDeepVerifySource,
    clearDeepVerifySources
  } = useResearchStore();

  // Get only enabled sources
  const enabledSources = deepVerifySourceConfigs.filter(s => s.enabled);

  const extractDomain = (url: string): string => {
    try {
      const urlObj = new URL(url);
      return urlObj.hostname.replace('www.', '');
    } catch {
      return 'unknown';
    }
  };

  const calculateRelevanceScore = (item: any, query: string): number => {
    const queryLower = query.toLowerCase();
    const title = (item.title || '').toLowerCase();
    const description = (item.description || '').toLowerCase();
    const markdown = (item.markdown || '').toLowerCase();
    
    let score = 0.5;
    
    // Title contains query terms
    const queryWords = queryLower.split(/\s+/).filter(w => w.length > 2);
    for (const word of queryWords) {
      if (title.includes(word)) score += 0.15;
      if (description.includes(word)) score += 0.1;
      if (markdown.includes(word)) score += 0.05;
    }
    
    // Exact phrase match
    if (title.includes(queryLower)) score += 0.2;
    if (description.includes(queryLower)) score += 0.1;
    
    return Math.min(1, score);
  };

  const generateReport = async (query: string, results: ResearchResult[]): Promise<Report> => {
    // Combine all content for AI analysis (explicit, source-grounded format)
    const combinedContent = results
      .slice(0, 8)
      .map((r, i) => `SOURCE ${i + 1}: ${r.url}\nTITLE: ${r.title}\nDOMAIN: ${r.metadata.domain || 'unknown'}\n\nCONTENT:\n${r.content}`)
      .join('\n\n---\n\n');

    let reportContent = '';
    
    try {
      // Generate AI-powered report
      const analyzeResult = await researchApi.analyze(query, combinedContent, 'report');
      
      if (analyzeResult.success && analyzeResult.result) {
        reportContent = analyzeResult.result;
      } else {
        // Fallback to basic report
        reportContent = generateBasicReport(query, results);
      }
    } catch (error) {
      console.error('AI report generation failed:', error);
      reportContent = generateBasicReport(query, results);
    }

    const sections = [
      { id: 'content', title: 'Full Report', content: reportContent, order: 1 },
    ];

    return {
      id: `report-${Date.now()}`,
      title: `Research Report: ${query}`,
      taskId: '',
      format: 'markdown',
      content: reportContent,
      createdAt: new Date(),
      sections,
    };
  };

  const generateBasicReport = (query: string, results: ResearchResult[]): string => {
    const uniqueDomains = new Set(results.map(r => r.metadata.domain)).size;
    const avgRelevance = Math.round(results.reduce((acc, r) => acc + r.relevanceScore, 0) / results.length * 100);
    
    return `# Research Report: ${query}

## Executive Summary

This comprehensive research report analyzes "${query}" using AI-powered web research. We examined ${results.length} high-quality sources from ${uniqueDomains} unique domains.

---

## Key Findings

${results.slice(0, 5).map((r, i) => `### ${i + 1}. ${r.title}

${r.summary}

**Source**: [${r.metadata.domain}](${r.url})  
**Relevance**: ${Math.round(r.relevanceScore * 100)}%
`).join('\n')}

---

## Data Summary

| Metric | Value |
|--------|-------|
| Total Sources | ${results.length} |
| Average Relevance | ${avgRelevance}% |
| Unique Domains | ${uniqueDomains} |

---

## All Sources

${results.map((r, i) => `${i + 1}. [${r.title}](${r.url})`).join('\n')}

---

*Generated by NexusAI Research Engine on ${new Date().toLocaleDateString()}*
`;
  };

  // Deep verify - crawl official sources first
  const crawlOfficialSources = useCallback(async (query: string, taskId: string): Promise<ResearchResult[]> => {
    const officialResults: ResearchResult[] = [];
    
    // Initialize sources status using enabled sources from config
    const initialSources = enabledSources.map(s => ({
      name: s.name,
      url: s.baseUrl,
      status: 'pending' as const,
      pagesFound: 0
    }));
    setDeepVerifySources(initialSources);
    
    for (const source of enabledSources) {
      try {
        // Update status to mapping
        updateDeepVerifySource(source.name, { status: 'mapping' });

        const mapResult = await researchApi.map(source.baseUrl, query, 50);
        
        if (!mapResult.success || !mapResult.links || mapResult.links.length === 0) {
          console.log(`No pages found for ${source.name} (${source.baseUrl})`);
          updateDeepVerifySource(source.name, { status: 'failed', pagesFound: 0 });
          continue;
        }

        // Filter URLs that might be relevant to the query
        const queryLower = query.toLowerCase();
        const relevantUrls = mapResult.links.filter((url: string) => {
          const urlLower = url.toLowerCase();
          // Check if URL contains relevant keywords
          return source.searchTerms.some(term => urlLower.includes(term.toLowerCase())) ||
                 urlLower.includes('ipo') ||
                 urlLower.includes('listing') ||
                 urlLower.includes('announce') ||
                 urlLower.includes('new') ||
                 urlLower.includes('2025') ||
                 urlLower.includes('2024');
        }).slice(0, 4); // Limit to 4 most relevant pages per source

        if (relevantUrls.length === 0) {
          // If no filtered results, take first few pages
          relevantUrls.push(...mapResult.links.slice(0, 2));
        }

        // Update status to scraping
        updateDeepVerifySource(source.name, { status: 'scraping', pagesFound: relevantUrls.length });

        // Scrape the relevant pages
        const scrapePromises = relevantUrls.map(async (url: string) => {
          try {
            const scrapeResult = await researchApi.scrape(url, ['markdown'], true, 3000);
            if (scrapeResult.success && scrapeResult.data?.markdown) {
              return {
                url,
                markdown: scrapeResult.data.markdown,
                title: scrapeResult.data.metadata?.title || url,
              };
            }
          } catch (e) {
            console.error(`Failed to scrape ${url}:`, e);
          }
          return null;
        });

        const scraped = await Promise.allSettled(scrapePromises);
        
        scraped.forEach((result, idx) => {
          if (result.status === 'fulfilled' && result.value) {
            const { url, markdown, title } = result.value;
            officialResults.push({
              id: `official-${Date.now()}-${idx}`,
              title: title,
              url: url,
              content: markdown,
              summary: markdown.substring(0, 300) + '...',
              relevanceScore: 0.95, // High relevance for official sources
              extractedAt: new Date(),
              metadata: {
                domain: extractDomain(url),
                wordCount: markdown.split(/\s+/).length,
              },
            });
          }
        });
        
        // Mark source as completed
        updateDeepVerifySource(source.name, { status: 'completed' });

      } catch (error) {
        console.error(`Error crawling ${source.baseUrl}:`, error);
        updateDeepVerifySource(source.name, { status: 'failed' });
      }
    }

    return officialResults;
  }, [enabledSources, setDeepVerifySources, updateDeepVerifySource]);

  const startResearch = useCallback(async (query: string) => {
    const taskId = `task-${Date.now()}`;
    
    const newTask: ResearchTask = {
      id: taskId,
      query,
      status: 'processing',
      progress: 0,
      results: [],
      createdAt: new Date(),
    };

    addTask(newTask);
    setIsSearching(true);

    try {
      let officialResults: ResearchResult[] = [];

      // Step 0: If Deep Verify mode is enabled, crawl official sources first
      if (deepVerifyMode) {
        updateTask(taskId, { progress: 5 });
        
        toast({
          title: "Deep Verify Mode Active",
          description: "Crawling official Saudi Exchange sources first...",
        });

        officialResults = await crawlOfficialSources(query, taskId);
        
        updateTask(taskId, { progress: 25 });
        
        toast({
          title: "Official Sources Crawled",
          description: `Found ${officialResults.length} pages from official sources`,
        });
      }

      // Step 1: Search the web (return URLs + snippets; we will scrape top results for accuracy)
      updateTask(taskId, { progress: deepVerifyMode ? 30 : 10 });

      toast({
        title: "Searching the web...",
        description: `Finding additional sources for: ${query}`,
      });

      const searchResult = await researchApi.search(query, deepVerifyMode ? 8 : 12, false);

      updateTask(taskId, { progress: deepVerifyMode ? 45 : 30 });

      if (!searchResult.success || !searchResult.data) {
        // If Deep Verify got results, continue with those
        if (officialResults.length > 0) {
          toast({
            title: "Web search failed",
            description: "Continuing with official sources only",
          });
        } else {
          throw new Error(searchResult.error || 'Search failed');
        }
      }

      // Step 2: Scrape top results for high-signal, page-level content
      toast({
        title: "Extracting source pages...",
        description: "Scraping top results for accuracy",
      });

      const searchData = searchResult.data || [];
      const baseResults: ResearchResult[] = searchData.map((item, index) => ({
        id: `result-${Date.now()}-${index}`,
        title: item.title || 'Untitled',
        url: item.url,
        content: item.description || '',
        summary: item.description || 'No summary available',
        relevanceScore: calculateRelevanceScore(item, query),
        extractedAt: new Date(),
        metadata: {
          domain: extractDomain(item.url),
          wordCount: 0,
        },
      }));

      // Sort by relevance before scraping
      baseResults.sort((a, b) => b.relevanceScore - a.relevanceScore);

      const isLowSignal = (md: string) => {
        const m = md.toLowerCase();
        return (
          m.includes('see all search results') ||
          m.includes('search result') ||
          (m.includes('tasi') && m.includes('nomu') && md.length > 20000 && m.includes('loader'))
        );
      };

      const toScrape = baseResults.slice(0, 6);

      // Scrape in parallel (best-effort)
      const scraped = await Promise.allSettled(
        toScrape.map(async (r) => {
          const first = await researchApi.scrape(r.url, ['markdown']);
          const md1 = first?.data?.markdown || '';
          if (first.success && md1 && !isLowSignal(md1)) {
            return { url: r.url, markdown: md1 };
          }
          // Retry with onlyMainContent=false for tricky sites
          const retry = await researchApi.scrape(r.url, ['markdown'], false);
          const md2 = retry?.data?.markdown || '';
          return { url: r.url, markdown: md2 || md1 };
        })
      );

      const scrapedByUrl = new Map<string, string>();
      scraped.forEach((p) => {
        if (p.status === 'fulfilled') {
          scrapedByUrl.set(p.value.url, p.value.markdown || '');
        }
      });

      const webResults: ResearchResult[] = baseResults.map((r) => {
        const md = scrapedByUrl.get(r.url);
        const content = md && md.trim().length > 200 ? md : r.content;
        return {
          ...r,
          content,
          summary: r.summary || (content ? content.substring(0, 300) + '...' : 'No summary available'),
          metadata: {
            ...r.metadata,
            wordCount: content ? content.split(/\s+/).length : 0,
          },
        };
      });

      // Combine official sources (prioritized) with web results
      const results: ResearchResult[] = [...officialResults, ...webResults];

      updateTask(taskId, { progress: 70, results });

      toast({
        title: "Generating grounded report...",
        description: "Only cited facts will be included",
      });

      // Sort by relevance
      results.sort((a, b) => b.relevanceScore - a.relevanceScore);

      updateTask(taskId, { progress: 75, results });

      // Step 3: Generate AI report
      toast({
        title: "Generating report...",
        description: "AI is analyzing the research data",
      });

      const report = await generateReport(query, results);
      
      updateTask(taskId, { progress: 100 });

      // Complete the task
      updateTask(taskId, {
        status: 'completed',
        progress: 100,
        results,
        completedAt: new Date(),
      });

      addReport({ ...report, taskId });
      setSearchQuery('');

      toast({
        title: "Research Complete",
        description: `Analyzed ${results.length} sources and generated a comprehensive report.`,
      });

      return { task: newTask, results, report };
    } catch (error) {
      console.error('Research error:', error);
      
      updateTask(taskId, {
        status: 'failed',
        progress: 0,
      });

      const errorMessage = error instanceof Error ? error.message : 'Unknown error';
      
      toast({
        title: "Research Failed",
        description: errorMessage,
        variant: "destructive",
      });

      throw error;
    } finally {
      setIsSearching(false);
    }
  }, [addTask, updateTask, addReport, setIsSearching, setSearchQuery, deepVerifyMode, crawlOfficialSources]);

  // Deep research - scrapes specific URLs
  const deepScrape = useCallback(async (url: string) => {
    toast({
      title: "Scraping...",
      description: `Extracting content from ${url}`,
    });

    const result = await researchApi.scrape(url);
    
    if (!result.success) {
      toast({
        title: "Scrape Failed",
        description: result.error || 'Failed to scrape URL',
        variant: "destructive",
      });
      return null;
    }

    toast({
      title: "Scrape Complete",
      description: `Successfully extracted content from ${extractDomain(url)}`,
    });

    return result;
  }, []);

  // Map a website to discover URLs
  const mapWebsite = useCallback(async (url: string, searchTerm?: string) => {
    toast({
      title: "Mapping website...",
      description: `Discovering URLs on ${url}`,
    });

    const result = await researchApi.map(url, searchTerm);
    
    if (!result.success) {
      toast({
        title: "Map Failed",
        description: result.error || 'Failed to map website',
        variant: "destructive",
      });
      return null;
    }

    toast({
      title: "Map Complete",
      description: `Found ${result.links?.length || 0} URLs`,
    });

    return result;
  }, []);

  return { 
    startResearch, 
    deepScrape, 
    mapWebsite,
    crawlOfficialSources
  };
};
